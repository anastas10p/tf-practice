{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Building a Recurrent Neural Network in TensorFlow 2.0.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z82c-Fcay0a3"
   },
   "source": [
    "## Part 0: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ynShOu8nNtFt"
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.6.2'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEjlM2EazOf0"
   },
   "source": [
    "## Part 1: Dataset preprocessing\n",
    "\n",
    "In this exercise, imdb reviews are classified by sentiment (positive/negative).\n",
    "For this, a Recurrent Neural Network is used.\n",
    "Also, the preprocessing of the data is quite different from previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB0tNtXJzTfA"
   },
   "source": [
    "### Set up dataset parameters\n",
    "\n",
    "These parameters determine what will be loaded from the dataset.\n",
    "Here, I go with the 20000 most frequent words in the dataset and set the maximum length of any sequence (converted review) to 100 words.\n",
    "For more details, see [the keras documentation page](https://keras.io/api/datasets/imdb/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jw6_KU24SrYK"
   },
   "source": [
    "number_of_words = 20000\n",
    "max_len = 100"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePywR8A4zaxT"
   },
   "source": [
    "### Load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6kCTV_hjOKmE"
   },
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=number_of_words)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZKDNoTKzi5w"
   },
   "source": [
    "### Pad all sequences to be the same length\n",
    "\n",
    "In order to classify the reviews, they have to be of the same length.\n",
    "To achieve this, they are padded, to achieve the max_len.\n",
    "Longer reviews are truncated."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LHcMNzv7Pd1s"
   },
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see a sequence. It will appear as a sequence of numbers and not of words, since the dataset has been partially prepared."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fcxd--ESP3Rh"
   },
   "source": [
    "X_train[1]"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  163,    11,  3215, 10156,     4,  1153,     9,   194,   775,\n           7,  8255, 11596,   349,  2637,   148,   605, 15358,  8003,\n          15,   123,   125,    68,     2,  6853,    15,   349,   165,\n        4362,    98,     5,     4,   228,     9,    43,     2,  1157,\n          15,   299,   120,     5,   120,   174,    11,   220,   175,\n         136,    50,     9,  4373,   228,  8255,     5,     2,   656,\n         245,  2350,     5,     4,  9837,   131,   152,   491,    18,\n           2,    32,  7464,  1212,    14,     9,     6,   371,    78,\n          22,   625,    64,  1382,     9,     8,   168,   145,    23,\n           4,  1690,    15,    16,     4,  1355,     5,    28,     6,\n          52,   154,   462,    33,    89,    78,   285,    16,   145,\n          95], dtype=int32)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xDMP44Zz0dU"
   },
   "source": [
    "### Set up Embedding Layer parameters\n",
    "\n",
    "For more information on the embedding layer see the next section."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nGHQ2upgQIGj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "cd40fe5e-bd42-442a-801a-55e79865ab8c"
   },
   "source": [
    "vocab_size = number_of_words\n",
    "vocab_size"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "20000"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PMyk2JcPQcjF"
   },
   "source": [
    "embed_size = 128"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG6LBKGnz7jT"
   },
   "source": [
    "## Part 2: Building a Recurrent Neural Network\n",
    "\n",
    "Recurrent neural networks differ greatly from simple ANNs and CNNs.\n",
    "For more details, see [the deep learning book](https://www.deeplearningbook.org/).\n",
    "\n",
    "The RNN consists (in an equivalent way to the CNN in 02) of a first part that makes the network a RNN and a simple part to give the output.\n",
    "There are more complex models which are outside of this exercise's scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUVnz-9K0DcW"
   },
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N2GHzwk6OMrV"
   },
   "source": [
    "model = tf.keras.Sequential()"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnXJZYR-0HXE"
   },
   "source": [
    "### Add the Embeding Layer\n",
    "\n",
    "Word embeddings are a method of \"translating\" words into numerical data for the neural networks.\n",
    "Word embeddings are dense and trainable, which means they are more efficient than one-hot encoders and allow the network to learn relationships between words.\n",
    "For more details see [here](https://www.tensorflow.org/text/guide/word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UWqC0DXbO9FU"
   },
   "source": [
    "model.add(tf.keras.layers.Embedding(vocab_size, embed_size, input_shape=(X_train.shape[1],)))"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM-lpTZX1mEG"
   },
   "source": [
    "### Add the LSTM Layer\n",
    "\n",
    "Long Short Term Memory neurons are very different than the neurons seen so far. They have logical gates which decide the pieces of information to be forgotten and the ones that are not.\n",
    "For more details see [this paper](https://arxiv.org/pdf/1506.02078.pdf).\n",
    "\n",
    "- number of neurons: 128\n",
    "- activation function: tanh"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5W7IXqhjQpAl",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "outputId": "ca4438c1-f0a3-4742-9cb6-803920134d18"
   },
   "source": [
    "model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T9M5Ult10XM"
   },
   "source": [
    "### Add the Dense output layer\n",
    "\n",
    "- neurons: 1\n",
    "- activation function: sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xe1nHzq7Q91-"
   },
   "source": [
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWcqM4Yr2ALS"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "This time I go with Root Mean Square Propagation for the optimizer.\n",
    "Recurrent neural networks run into gradient decay problems (where the gradient disappears), and rmprop was developed to overcome this problem."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-z9ACOXcRUUN"
   },
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PiolKKO6RjVF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "outputId": "6b165bfd-2c4d-434a-8bc1-42cc6ba91720"
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bPUvbfe2GJI"
   },
   "source": [
    "## Part 3: Train the model\n",
    "\n",
    "The parameter batch size is used to define the size of training batches, i.e. the amount of data samples put into the network.\n",
    "Batch size is an important optimizable training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9FqUTA1CRpQ8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "outputId": "0d496788-f042-4fb3-d7d9-18166d2cbc9d"
   },
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=128)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "196/196 [==============================] - 89s 430ms/step - loss: 0.4620 - accuracy: 0.7829\n",
      "Epoch 2/3\n",
      "196/196 [==============================] - 90s 460ms/step - loss: 0.2896 - accuracy: 0.8836\n",
      "Epoch 3/3\n",
      "196/196 [==============================] - 91s 462ms/step - loss: 0.2371 - accuracy: 0.9067\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f1b7f0da9b0>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GJ4irh1bCX7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wMo2wYpbCgb"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a8kD_6q-RySO",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "4f792319-1095-42b8-c35a-fb8f2f5093e6"
   },
   "source": [
    "test_loss, test_acurracy = model.evaluate(X_test, y_test)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 55s 69ms/step - loss: 0.3677 - accuracy: 0.8455\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C0XnUtS-cEeI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "0d99df5f-717e-4751-c4dc-48e77d765056",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(\"Test accuracy: {}\".format(test_acurracy))"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8454800248146057\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 4: Save the model\n",
    "\n",
    "### Save the architecture of the network as .json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"03_imdb_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the network weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model.save_weights(\"03_imdb_model.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}